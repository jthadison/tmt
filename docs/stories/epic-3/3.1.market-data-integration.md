# Story 3.1: Market Data Integration Pipeline

## Status
Draft

## Story
**As a** trading system,
**I want** real-time market data from multiple sources,
**so that** I can analyze price action and generate signals.

## Acceptance Criteria
1. WebSocket connections to OANDA, Polygon.io for real-time forex/index data
2. Data normalized into standard OHLCV format with <50ms processing time
3. Automatic reconnection with data gap recovery on disconnection
4. Historical data backfill for 2 years of 1-minute candles
5. Market data stored in TimescaleDB with efficient time-series queries
6. Data quality monitoring detects gaps, spikes, and anomalies

## Tasks / Subtasks
- [ ] Task 1: Implement OANDA API integration (AC: 1)
  - [ ] Create OANDA WebSocket client for real-time price streaming
  - [ ] Implement REST API client for historical data and account info
  - [ ] Configure authentication with Bearer token and API key management
  - [ ] Handle rate limits: 120 requests/minute for REST, unlimited WebSocket
  - [ ] Add support for major forex pairs (EUR/USD, GBP/USD, USD/JPY, etc.)
  - [ ] Implement subscription management for multiple instruments
- [ ] Task 2: Implement Polygon.io API integration (AC: 1)
  - [ ] Create Polygon.io WebSocket client for real-time index data
  - [ ] Implement REST API client for historical aggregates
  - [ ] Configure authentication with API key in headers/query params
  - [ ] Handle rate limits: 5-1000 calls/minute based on plan
  - [ ] Add support for major indices (US30, NAS100, SPX500)
  - [ ] Implement failover from Polygon to OANDA for forex data
- [ ] Task 3: Build data normalization engine (AC: 2)
  - [ ] Create standard OHLCV format with timestamp, open, high, low, close, volume
  - [ ] Implement data transformation from OANDA format to standard format
  - [ ] Implement data transformation from Polygon.io format to standard format
  - [ ] Add data validation and sanitization (remove invalid prices/volumes)
  - [ ] Optimize processing pipeline for <50ms latency requirement
  - [ ] Add data deduplication to handle duplicate ticks
- [ ] Task 4: Implement connection resilience (AC: 3)
  - [ ] Create automatic reconnection logic with exponential backoff
  - [ ] Implement data gap detection by monitoring timestamp sequences
  - [ ] Build gap recovery system using REST API historical data
  - [ ] Add connection health monitoring with heartbeat checks
  - [ ] Create failover mechanism between data sources
  - [ ] Implement circuit breaker for persistent connection failures
- [ ] Task 5: Build historical data backfill system (AC: 4)
  - [ ] Create batch downloader for 2 years of 1-minute OHLCV data
  - [ ] Implement parallel downloading with rate limit compliance
  - [ ] Add data integrity verification (check for gaps, validate ranges)
  - [ ] Create incremental backfill for missing historical periods
  - [ ] Implement data compression for storage efficiency
  - [ ] Add progress monitoring and resume capability for large downloads
- [ ] Task 6: Setup TimescaleDB storage (AC: 5)
  - [ ] Create TimescaleDB hypertable for market_data with time partitioning
  - [ ] Implement efficient batch insertion with COPY commands
  - [ ] Create indexes for symbol, timeframe, and time-range queries
  - [ ] Configure data retention policies and compression
  - [ ] Add query optimization for common time-series patterns
  - [ ] Implement real-time streaming insertion with buffering
- [ ] Task 7: Implement data quality monitoring (AC: 6)
  - [ ] Create gap detection algorithm for missing time periods
  - [ ] Implement spike detection using statistical outlier analysis
  - [ ] Add anomaly detection for volume and price inconsistencies
  - [ ] Create data quality metrics dashboard
  - [ ] Implement automated alerts for data quality issues
  - [ ] Add data quality reporting with statistics and trends

## Dev Notes

### Architecture Context
The Market Data Integration Pipeline serves as the foundation for all trading analysis, providing real-time and historical market data to the Market Analysis Agent and other system components. The pipeline must handle multiple data sources with sub-50ms latency while maintaining data quality and resilience. TimescaleDB is used for efficient time-series storage and querying. [Source: architecture/high-level-architecture.md#timescaledb-time-series]

### Previous Epic Context
Epics 1-2 established the infrastructure foundation with Kafka messaging, monitoring, and multi-account management. The Market Data Pipeline integrates with these systems by publishing market data events to Kafka and using the monitoring infrastructure for data quality tracking.

### OANDA API Integration Details
Based on the external API specifications:
- **Primary data source** for forex pairs (EUR/USD, GBP/USD, USD/JPY, etc.)
- **Authentication:** Bearer token with API key stored in HashiCorp Vault
- **Rate limits:** 120 requests/minute for REST, unlimited WebSocket connections
- **WebSocket endpoint:** `/v3/pricing/stream` for real-time price streaming
- **Historical data:** `/v3/instruments/{instrument}/candles` for OHLCV data

Connection implementation:
```python
class OANDAClient:
    def __init__(self, api_key, environment='practice'):
        self.base_url = 'https://api-fxpractice.oanda.com' if environment == 'practice' else 'https://api-fxtrade.oanda.com'
        self.headers = {'Authorization': f'Bearer {api_key}'}
        
    async def stream_prices(self, instruments):
        # WebSocket implementation for real-time data
        # Handle reconnection with exponential backoff
        # Implement data gap detection and recovery
```

[Source: architecture/external-apis.md#oanda-api]

### Polygon.io API Integration Details
- **Backup data source** and primary for indices (US30, NAS100, SPX500)
- **Authentication:** API key via query parameter or header
- **Rate limits:** 5-1000 calls/minute based on subscription plan
- **WebSocket endpoint:** `wss://socket.polygon.io/` for real-time market data
- **Historical data:** `/v2/aggs/ticker/{ticker}/range/` for aggregated OHLCV

Failover logic implementation:
```python
class MarketDataManager:
    def __init__(self):
        self.oanda_client = OANDAClient()
        self.polygon_client = PolygonClient()
        
    async def get_market_data(self, symbol):
        try:
            # Try primary source first
            if symbol in FOREX_PAIRS:
                return await self.oanda_client.get_data(symbol)
            else:
                return await self.polygon_client.get_data(symbol)
        except ConnectionError:
            # Fallback to secondary source
            return await self.get_fallback_data(symbol)
```

### Data Normalization Schema
Standard OHLCV format for all data sources:
```python
@dataclass
class MarketTick:
    symbol: str
    timestamp: datetime
    timeframe: str  # '1m', '5m', '1h', '4h', '1d'
    open: Decimal
    high: Decimal
    low: Decimal
    close: Decimal
    volume: int
    source: str  # 'oanda', 'polygon'
    
    def to_dict(self):
        return {
            'symbol': self.symbol,
            'timestamp': self.timestamp.isoformat(),
            'timeframe': self.timeframe,
            'open': float(self.open),
            'high': float(self.high),
            'low': float(self.low),
            'close': float(self.close),
            'volume': self.volume,
            'source': self.source
        }
```

### TimescaleDB Schema Implementation
Based on the database schema:
```sql
CREATE TABLE market_data (
    time TIMESTAMP WITH TIME ZONE NOT NULL,
    symbol VARCHAR(20) NOT NULL,
    timeframe VARCHAR(10) NOT NULL,
    open DECIMAL(10,5) NOT NULL,
    high DECIMAL(10,5) NOT NULL,
    low DECIMAL(10,5) NOT NULL,
    close DECIMAL(10,5) NOT NULL,
    volume BIGINT NOT NULL,
    source VARCHAR(20) NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Create hypertable with 1-day chunks
SELECT create_hypertable('market_data', 'time', chunk_time_interval => INTERVAL '1 day');

-- Create indexes for efficient queries
CREATE INDEX idx_market_data_symbol_time ON market_data (symbol, time DESC);
CREATE INDEX idx_market_data_timeframe ON market_data (timeframe, time DESC);
```

### Data Quality Monitoring Implementation
Comprehensive monitoring for data integrity:

**Gap Detection:**
```python
def detect_gaps(symbol, timeframe, start_time, end_time):
    """Detect missing time periods in market data"""
    expected_intervals = generate_expected_intervals(start_time, end_time, timeframe)
    actual_data = query_market_data(symbol, timeframe, start_time, end_time)
    gaps = expected_intervals - actual_data.timestamps
    return gaps
```

**Spike Detection:**
```python
def detect_spikes(data, threshold=3.0):
    """Detect price spikes using standard deviation analysis"""
    price_changes = np.diff(data['close'])
    mean_change = np.mean(price_changes)
    std_change = np.std(price_changes)
    
    spikes = []
    for i, change in enumerate(price_changes):
        if abs(change - mean_change) > threshold * std_change:
            spikes.append({
                'timestamp': data['timestamp'][i+1],
                'price': data['close'][i+1],
                'change': change,
                'severity': abs(change - mean_change) / std_change
            })
    return spikes
```

### Performance Requirements
Critical latency and throughput targets:
- **Processing latency:** <50ms from raw data to normalized format
- **Storage latency:** <100ms for TimescaleDB insertion
- **Query performance:** <200ms for 1-year historical queries
- **Throughput:** Support 100+ symbols with 1-second tick updates
- **Recovery time:** <30 seconds to detect and recover from connection loss

### Integration with Market Analysis Agent
The Market Data Pipeline integrates with downstream components:
- **Kafka events:** Publishes `market.data.tick` events for real-time analysis
- **Analysis Agent:** Provides data for Wyckoff pattern detection
- **Storage queries:** Supports historical data queries for backtesting
- **Monitoring:** Integrates with Prometheus metrics for system health

Event format:
```json
{
  "event_type": "market.data.tick",
  "symbol": "EURUSD",
  "timestamp": "2024-01-01T10:00:00Z",
  "data": {
    "open": 1.0950,
    "high": 1.0955,
    "low": 1.0948,
    "close": 1.0952,
    "volume": 1250000,
    "timeframe": "1m",
    "source": "oanda"
  },
  "correlation_id": "uuid"
}
```

### File Structure
Based on the source tree architecture:
```
agents/market-analysis/
├── app/
│   ├── market_data/
│   │   ├── __init__.py
│   │   ├── oanda_client.py      # OANDA API integration
│   │   ├── polygon_client.py    # Polygon.io integration
│   │   ├── data_normalizer.py   # OHLCV normalization
│   │   ├── gap_recovery.py      # Data gap handling
│   │   └── quality_monitor.py   # Data quality monitoring
│   ├── storage/
│   │   ├── timescale_client.py  # TimescaleDB operations
│   │   └── data_retention.py    # Retention policies
│   └── main.py
```

### Testing Requirements
- **API Tests:** Mock OANDA and Polygon.io responses for unit testing
- **Integration Tests:** Test with real API connections in staging
- **Performance Tests:** Validate <50ms processing latency requirement
- **Resilience Tests:** Test reconnection and gap recovery scenarios
- **Data Quality Tests:** Validate gap detection and spike identification
- **Load Tests:** Handle 100+ symbols with high-frequency updates

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-08-06 | 1.0 | Initial story creation | Bob (Scrum Master) |

## Dev Agent Record
*This section will be populated by the development agent during implementation*

### Agent Model Used
*To be filled by dev agent*

### Debug Log References  
*To be filled by dev agent*

### Completion Notes List
*To be filled by dev agent*

### File List
*To be filled by dev agent*

## QA Results
*Results from QA Agent review will be populated here*