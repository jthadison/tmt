# Story 1.4: Inter-Agent Communication Infrastructure

## Status
Draft

## Story
**As a** system architect,
**I want** reliable message passing between agents,
**so that** the multi-agent system can coordinate effectively.

## Acceptance Criteria
1. Kafka/NATS message broker deployed and configured
2. Event schema defined for all agent communication types
3. Message delivery guarantees implemented (at-least-once delivery)
4. Dead letter queue for failed messages
5. Message latency monitoring shows <10ms average between agents
6. Test harness created for simulating agent communication

## Tasks / Subtasks
- [ ] Task 1: Deploy and configure Kafka message broker (AC: 1)
  - [ ] Add Kafka 3.6.1 service to docker-compose.yml
  - [ ] Configure Kafka brokers with appropriate replication and partitioning
  - [ ] Set up Zookeeper for Kafka cluster management
  - [ ] Configure Kafka for development and production environments
  - [ ] Add Kafka connectivity to shared agent utilities
- [ ] Task 2: Define event schemas and message formats (AC: 2)
  - [ ] Create event schema definitions for all agent communication types
  - [ ] Define standardized message format with correlation IDs
  - [ ] Create signal generation events (trading.signals.generated)
  - [ ] Create risk management events (risk.parameters.updated)
  - [ ] Create circuit breaker events (breaker.agent.triggered, breaker.system.emergency)
  - [ ] Create personality variance events (personality.variance.applied)
  - [ ] Create execution events (execution.order.placed, execution.order.filled)
- [ ] Task 3: Implement message delivery guarantees (AC: 3)
  - [ ] Configure Kafka for at-least-once delivery semantics
  - [ ] Implement producer acknowledgment settings (acks=all)
  - [ ] Configure consumer group settings for guaranteed processing
  - [ ] Add idempotency handling for duplicate message protection
  - [ ] Implement message deduplication using correlation IDs
- [ ] Task 4: Setup dead letter queue and error handling (AC: 4)
  - [ ] Configure dead letter queue topics for failed message processing
  - [ ] Implement retry logic with exponential backoff
  - [ ] Add message poison detection and isolation
  - [ ] Configure dead letter queue monitoring and alerting
  - [ ] Create dead letter queue consumer for manual message recovery
- [ ] Task 5: Implement latency monitoring (AC: 5)
  - [ ] Add message timing instrumentation to producers and consumers
  - [ ] Configure Prometheus metrics for message latency tracking
  - [ ] Set up monitoring to ensure <10ms average latency between agents
  - [ ] Add alerts for latency threshold violations
  - [ ] Create Grafana dashboards for message flow visualization
- [ ] Task 6: Create shared Kafka client utilities (AC: 6)
  - [ ] Implement shared kafka_producer.py in /shared/python-utils/messaging/
  - [ ] Implement shared kafka_consumer.py with consumer group management
  - [ ] Add connection pooling and resource management
  - [ ] Create test harness for simulating agent communication
  - [ ] Add integration tests for message flow between agents

## Dev Notes

### Architecture Context
The inter-agent communication system uses Apache Kafka 3.6.1 for event streaming with durability guarantees and exactly-once semantics. The system implements an event-driven microservices architecture where agents communicate asynchronously via message queues to ensure resilience and prevent cascade failures. [Source: architecture/tech-stack.md#technology-stack-table, architecture/high-level-architecture.md#event-driven-architecture]

### Previous Story Context
Stories 1.1-1.3 established the repository structure with `/shared/python-utils/messaging/` directory, the Circuit Breaker Agent that needs to consume health events from other agents, and the Docker Compose infrastructure that can host the Kafka message broker.

### Kafka Configuration Requirements
Based on the technology stack and high-level architecture:
- **Apache Kafka 3.6.1** for event streaming
- **Durability guarantees:** exactly-once semantics for financial transactions
- **Message latency target:** <10ms average between agents for the signal-to-execution pipeline
- **Integration:** All agents must use shared Kafka client utilities for consistency

The Kafka broker must be configured in Docker Compose for local development and integrated with Google Cloud Pub/Sub equivalent in production GKE deployment. [Source: architecture/tech-stack.md]

### Event Schema Design
The system requires standardized event schemas for agent coordination. Based on core workflows analysis:

**Critical Event Types:**
- `trading.signals.generated` - Market Analysis Agent to Risk Agent
- `risk.parameters.updated` - Risk Agent broadcasting updated parameters
- `breaker.agent.triggered` - Circuit Breaker Agent-level stops
- `breaker.system.emergency` - Circuit Breaker system-wide halt
- `personality.variance.applied` - Personality Engine execution variance
- `execution.order.placed` - Execution Engine order placement
- `execution.order.filled` - Execution Engine order completion

[Source: architecture/core-workflows.md#signal-generation-and-trade-execution-workflow]

### Message Format Standards
All messages must follow the established coding standards:
- **Correlation ID:** Every Kafka message must include correlation_id for distributed tracing
- **Message Format:** Structured JSON with consistent envelope format
- **Idempotency:** Use UUID idempotency keys with 24-hour retention
- **Timeout Configuration:** Message processing must have 30s timeout for database operations

[Source: architecture/coding-standards.md#critical-rules]

### Delivery Guarantees Implementation
Kafka configuration must provide financial-grade reliability:
- **At-least-once delivery** with consumer acknowledgments
- **Producer settings:** `acks=all` for guaranteed write acknowledgment
- **Consumer settings:** Manual commit after successful processing
- **Retry policy:** Exponential backoff with jitter: 1s, 2s, 4s, 8s, then dead letter queue
- **Idempotency:** Handle duplicate messages using correlation IDs and database constraints

[Source: architecture/error-handling-strategy.md#external-api-errors]

### Shared Client Implementation
The messaging utilities must be implemented in the shared Python utilities:
```
shared/python-utils/messaging/
├── __init__.py
├── kafka_producer.py      # Shared producer with connection pooling
└── kafka_consumer.py      # Shared consumer with group management
```

All agents must use these shared clients for consistency in:
- Connection management and pooling
- Error handling and retry logic
- Correlation ID propagation
- Metrics collection and monitoring

[Source: architecture/source-tree.md#shared-python-utils]

### Integration with Agent Architecture
Each agent service must integrate Kafka messaging through:
- **Base Agent:** Shared base_agent.py must include Kafka client initialization
- **Health Monitoring:** Circuit Breaker Agent consumes health events from all agents
- **Signal Flow:** Market Analysis → Risk Management → Personality Engine → Execution
- **Feedback Loops:** Execution results feed back to Learning Agent for adaptation

The messaging system enables the multi-agent coordination required for the 8 specialized AI agents. [Source: architecture/components.md]

### Performance Requirements
The system must achieve ultra-low latency for the critical signal-to-execution path:
- **Target Latency:** <10ms average between agents
- **Critical Path:** Signal detection to execution must be <100ms total
- **Monitoring:** Prometheus metrics must track message latency percentiles
- **Alerting:** Violations of latency thresholds must trigger immediate alerts

[Source: Epic 1.4 AC5]

### Testing Strategy
Message infrastructure requires comprehensive testing:
- **Unit Tests:** Individual producer/consumer functionality
- **Integration Tests:** Multi-agent message flows with real Kafka
- **Performance Tests:** Latency and throughput under load
- **Failure Tests:** Network partitions, broker failures, poison messages
- **Test Harness:** Simulate all agent communication patterns for development

[Source: architecture/test-strategy-and-standards.md#integration-tests]

### Error Handling Integration
Message handling must integrate with the established error handling strategy:
- **Structured Logging:** All message processing uses JSON logging with correlation IDs
- **Exception Hierarchy:** Message errors inherit from base `TradingSystemError`
- **Dead Letter Queue:** Failed messages route to DLQ for manual investigation
- **Circuit Breakers:** Message processing failures contribute to circuit breaker monitoring

[Source: architecture/error-handling-strategy.md]

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-08-06 | 1.0 | Initial story creation | Bob (Scrum Master) |

## Dev Agent Record
*This section will be populated by the development agent during implementation*

### Agent Model Used
*To be filled by dev agent*

### Debug Log References  
*To be filled by dev agent*

### Completion Notes List
*To be filled by dev agent*

### File List
*To be filled by dev agent*

## QA Results
*Results from QA Agent review will be populated here*