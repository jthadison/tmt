"""
Shadow testing framework for parameter optimization validation.

Implements A/B testing framework with 10% signal allocation to test group,
statistical significance testing using t-tests, and automated deployment decisions.
"""

import logging
import random
import uuid
from datetime import datetime, timedelta
from decimal import Decimal
from typing import Dict, List, Optional

from scipy import stats

from .models.suggestion_models import ParameterSuggestion, TestEvaluation


logger = logging.getLogger(__name__)


class ShadowTester:
    """
    Shadow testing framework for parameter validation.

    Allocates 10% of signals to test parameter variations, tracks performance
    metrics for control and test groups, and uses statistical significance
    testing to make deployment decisions.
    """

    def __init__(
        self,
        shadow_test_repository,
        trade_repository,
        allocation_percentage: float = 0.10,
        min_duration_days: int = 7,
        min_sample_size: int = 30,
        max_p_value: float = 0.05,
        min_improvement_pct: float = 20.0
    ):
        """
        Initialize shadow testing framework.

        Args:
            shadow_test_repository: Repository for shadow test persistence
            trade_repository: Repository for trade data queries
            allocation_percentage: Percentage of signals allocated to test group (default: 0.10)
            min_duration_days: Minimum test duration before evaluation (default: 7)
            min_sample_size: Minimum test trades required for deployment (default: 30)
            max_p_value: Maximum p-value for statistical significance (default: 0.05)
            min_improvement_pct: Minimum improvement for deployment (default: 20.0)
        """
        self.shadow_test_repository = shadow_test_repository
        self.trade_repository = trade_repository
        self.allocation_percentage = allocation_percentage
        self.min_duration_days = min_duration_days
        self.min_sample_size = min_sample_size
        self.max_p_value = max_p_value
        self.min_improvement_pct = min_improvement_pct

        logger.info(
            f"âœ… ShadowTester initialized "
            f"(allocation: {allocation_percentage * 100}%, "
            f"min_duration: {min_duration_days} days, "
            f"min_sample: {min_sample_size})"
        )

    async def start_test(
        self,
        suggestion: ParameterSuggestion,
        allocation: Optional[float] = None,
        duration_days: Optional[int] = None
    ) -> str:
        """
        Start a shadow test for a parameter suggestion.

        Args:
            suggestion: Parameter suggestion to test
            allocation: Signal allocation percentage (default: use instance value)
            duration_days: Test duration in days (default: use instance value)

        Returns:
            str: Unique test ID

        Raises:
            Exception: If test creation fails
        """
        # Generate unique test ID
        test_id = str(uuid.uuid4())

        # Use provided values or defaults
        allocation_pct = allocation if allocation is not None else self.allocation_percentage
        min_duration = duration_days if duration_days is not None else self.min_duration_days

        # Create test data
        test_data = {
            "test_id": test_id,
            "suggestion_id": suggestion.suggestion_id,
            "parameter_name": suggestion.parameter,
            "session": suggestion.session,
            "current_value": suggestion.current_value,
            "test_value": suggestion.suggested_value,
            "start_date": datetime.now(),
            "min_duration_days": min_duration,
            "allocation_pct": Decimal(str(allocation_pct * 100)),  # Store as percentage
            "status": "ACTIVE"
        }

        # Save test to database
        await self.shadow_test_repository.create_test(test_data)

        # Update suggestion status to TESTING
        suggestion.status = "TESTING"

        logger.info(
            f"ðŸ§ª Started shadow test {test_id} for {suggestion.parameter} "
            f"on {suggestion.session} session "
            f"({suggestion.current_value} â†’ {suggestion.suggested_value})"
        )

        return test_id

    async def allocate_signal_to_group(self, signal: Dict) -> str:
        """
        Allocate signal to TEST or CONTROL group.

        Uses random allocation with configured allocation percentage to
        assign signals to test or control group.

        Args:
            signal: Signal dictionary containing session and other metadata

        Returns:
            str: "TEST" or "CONTROL" group assignment
        """
        # Get active tests for this signal's session
        active_tests = await self.shadow_test_repository.get_active_tests()

        # Filter tests for this session
        signal_session = signal.get("session", "").upper()
        relevant_tests = [
            test for test in active_tests
            if test.session == signal_session or test.session == "ALL"
        ]

        if not relevant_tests:
            # No active tests, all signals go to control
            return "CONTROL"

        # Random allocation (10% to TEST, 90% to CONTROL)
        random_value = random.random()

        if random_value < self.allocation_percentage:
            return "TEST"
        else:
            return "CONTROL"

    async def record_trade_result(
        self,
        test_id: str,
        group: str,
        trade: Dict
    ) -> None:
        """
        Record trade result for control or test group.

        Updates trade counters and recalculates performance metrics
        for the specified group.

        Args:
            test_id: Shadow test ID
            group: "CONTROL" or "TEST"
            trade: Trade dictionary with pnl, risk_reward_ratio, etc.

        Raises:
            Exception: If test not found or database operation fails
        """
        # Get current test
        test = await self.shadow_test_repository.get_test_by_id(test_id)

        if not test:
            logger.warning(f"âš ï¸  Shadow test not found: {test_id}")
            return

        # Query all trades for this test to recalculate metrics
        # Note: In production, trades would be tagged with test_id and group
        # For now, we'll increment counters and update metrics incrementally

        if group == "CONTROL":
            # Increment control counter
            new_control_trades = test.control_trades + 1

            # Recalculate control metrics (simplified incremental update)
            # In production, query all control trades for accurate calculation
            metrics = {
                "control_trades": new_control_trades
            }

            # Update win rate if trade has P&L
            if "pnl" in trade and trade["pnl"] is not None:
                # This is a simplified update - production should recalculate from all trades
                pass

        elif group == "TEST":
            # Increment test counter
            new_test_trades = test.test_trades + 1

            # Recalculate test metrics
            metrics = {
                "test_trades": new_test_trades
            }

        else:
            logger.error(f"âŒ Invalid group: {group}")
            return

        # Update test metrics in database
        await self.shadow_test_repository.update_test_metrics(test_id, metrics)

        logger.debug(f"ðŸ“Š Recorded {group} trade for test {test_id}")

    async def evaluate_test(self, test_id: str) -> TestEvaluation:
        """
        Evaluate shadow test performance and make deployment decision.

        Performs statistical significance testing using t-test to compare
        control and test group performance.

        Args:
            test_id: Shadow test ID

        Returns:
            TestEvaluation: Evaluation results with deployment decision

        Raises:
            Exception: If test not found or evaluation fails
        """
        # Get test from database
        test = await self.shadow_test_repository.get_test_by_id(test_id)

        if not test:
            raise ValueError(f"Shadow test not found: {test_id}")

        # Check test duration
        test_duration = (datetime.now() - test.start_date).days

        if test_duration < test.min_duration_days:
            logger.info(
                f"â„¹ï¸  Test {test_id} insufficient duration: "
                f"{test_duration} < {test.min_duration_days} days"
            )
            return TestEvaluation(
                test_id=test_id,
                should_deploy=False,
                improvement_pct=Decimal("0"),
                p_value=Decimal("1.0"),
                control_mean_pnl=Decimal("0"),
                test_mean_pnl=Decimal("0"),
                control_win_rate=Decimal("0"),
                test_win_rate=Decimal("0"),
                sample_size_control=test.control_trades,
                sample_size_test=test.test_trades,
                reason=f"Insufficient duration: {test_duration} < {test.min_duration_days} days"
            )

        # Check test sample size
        if test.test_trades < self.min_sample_size:
            logger.info(
                f"â„¹ï¸  Test {test_id} insufficient sample: "
                f"{test.test_trades} < {self.min_sample_size} trades"
            )
            return TestEvaluation(
                test_id=test_id,
                should_deploy=False,
                improvement_pct=Decimal("0"),
                p_value=Decimal("1.0"),
                control_mean_pnl=Decimal("0"),
                test_mean_pnl=Decimal("0"),
                control_win_rate=Decimal("0"),
                test_win_rate=Decimal("0"),
                sample_size_control=test.control_trades,
                sample_size_test=test.test_trades,
                reason=f"Insufficient data: {test.test_trades} < {self.min_sample_size} trades"
            )

        # Query control and test trade results
        # Note: In production, trades would be filtered by test_id and group
        # For this implementation, we'll use synthetic data from test metrics
        control_pnl_list = await self._get_trade_pnl_list(test_id, "CONTROL")
        test_pnl_list = await self._get_trade_pnl_list(test_id, "TEST")

        # Handle case where we don't have actual trade data yet
        if not control_pnl_list or not test_pnl_list:
            logger.warning(
                f"âš ï¸  No trade data available for test {test_id}, "
                "using placeholder metrics"
            )
            # Use placeholder values for now
            control_mean_pnl = Decimal("100")
            test_mean_pnl = Decimal("100")
            p_value = Decimal("1.0")
            improvement_pct = Decimal("0")
        else:
            # Calculate mean P&L
            control_mean_pnl = Decimal(str(sum(control_pnl_list) / len(control_pnl_list)))
            test_mean_pnl = Decimal(str(sum(test_pnl_list) / len(test_pnl_list)))

            # Calculate improvement percentage
            if control_mean_pnl != 0:
                improvement_pct = ((test_mean_pnl - control_mean_pnl) / abs(control_mean_pnl)) * Decimal("100")
            else:
                improvement_pct = Decimal("0")

            # Perform t-test for statistical significance
            if len(control_pnl_list) >= 2 and len(test_pnl_list) >= 2:
                statistic, p_value_float = stats.ttest_ind(control_pnl_list, test_pnl_list)
                p_value = Decimal(str(p_value_float))
            else:
                p_value = Decimal("1.0")  # Not enough data for t-test

        # Get win rates from test metrics
        control_win_rate = test.control_win_rate or Decimal("0")
        test_win_rate = test.test_win_rate or Decimal("0")

        # Make deployment decision
        should_deploy = (
            improvement_pct >= Decimal(str(self.min_improvement_pct))
            and p_value < Decimal(str(self.max_p_value))
        )

        # Generate reason
        if should_deploy:
            reason = (
                f"Deploy recommended: {improvement_pct:.1f}% improvement "
                f"with p-value {p_value:.4f} (significant)"
            )
        elif p_value >= Decimal(str(self.max_p_value)):
            reason = (
                f"Not statistically significant: p-value {p_value:.4f} >= {self.max_p_value}"
            )
        elif improvement_pct <= Decimal(str(self.min_improvement_pct)):
            reason = (
                f"Insufficient improvement: {improvement_pct:.1f}% <= {self.min_improvement_pct}%"
            )
        else:
            reason = "Evaluation criteria not met"

        evaluation = TestEvaluation(
            test_id=test_id,
            should_deploy=should_deploy,
            improvement_pct=improvement_pct,
            p_value=p_value,
            control_mean_pnl=control_mean_pnl,
            test_mean_pnl=test_mean_pnl,
            control_win_rate=control_win_rate,
            test_win_rate=test_win_rate,
            sample_size_control=test.control_trades,
            sample_size_test=test.test_trades,
            reason=reason
        )

        # Update test status to COMPLETED with evaluation results
        await self.shadow_test_repository.complete_test(
            test_id,
            {
                "improvement_pct": improvement_pct,
                "p_value": p_value
            }
        )

        logger.info(
            f"âœ… Evaluated test {test_id}: "
            f"{'DEPLOY' if should_deploy else 'NO DEPLOY'} "
            f"({improvement_pct:.1f}% improvement, p={p_value:.4f})"
        )

        return evaluation

    async def terminate_test(self, test_id: str, reason: str) -> None:
        """
        Terminate shadow test early.

        Stops signal allocation and marks test as terminated.

        Args:
            test_id: Shadow test ID
            reason: Termination reason

        Raises:
            Exception: If test not found or database operation fails
        """
        await self.shadow_test_repository.terminate_test(test_id, reason)

        logger.info(f"ðŸ›‘ Terminated shadow test {test_id}: {reason}")

    async def get_completed_tests(self) -> List:
        """
        Get all completed shadow tests ready for evaluation.

        Returns:
            List[ShadowTest]: List of completed shadow tests
        """
        return await self.shadow_test_repository.get_completed_tests()

    async def _get_trade_pnl_list(self, test_id: str, group: str) -> List[float]:
        """
        Get P&L values for all trades in specified group.

        Note: This is a placeholder implementation. In production, trades
        would be tagged with test_id and group, and this method would
        query the trade repository.

        Args:
            test_id: Shadow test ID
            group: "CONTROL" or "TEST"

        Returns:
            List[float]: List of P&L values
        """
        # Placeholder implementation
        # In production, query trade_repository with filters:
        # trades = await self.trade_repository.get_trades_by_test(test_id, group)
        # return [float(trade.pnl) for trade in trades if trade.pnl]

        # For now, return empty list
        # Tests will inject mock data
        return []
